👋 Hi, I'm Albert Glenn

💾 Data Engineering | ⚙️ Pipeline Performance & Cost Optimization | ☁️ Cloud + Streaming | 📈 Data for Business Impact

🚀 About Me

I specialize in building scalable, secure, and cost-efficient data systems that transform raw data into trusted assets for analytics, machine learning, and product innovation.

My unique edge:
I engineer pipelines that reduce warehouse compute + storage spend while improving performance and data availability.

Currently completing the Zero to Mastery Data Engineering Career Path and applying every concept through real-world projects in healthcare and finance.

💡 Business Impact Engineering

I architect pipelines that deliver faster insights at lower cost:

📉 Reduced compute/storage costs with Delta Lake optimization
(partitioning, Z-ordering, file compaction, caching)

⚡ Faster ETL + query performance → less runtime, fewer cluster hours

🔁 Reuse data efficiently via ELT + bronze/silver/gold layers

🔐 Governance + schema enforcement → higher quality & compliance

🚀 Improved analytics/ML velocity → quicker data-driven decisions

My priority: enable teams to do more with data while spending less.

🧱 What I Build

✅ Batch + streaming data pipelines (Kafka, Spark, S3)
✅ Data lakes with Delta format & Lakehouse architecture (Databricks + AWS)
✅ Data modeling: OLTP + OLAP + star schemas
✅ Secure + governed environments (RBAC, .env, IAM)
✅ Power BI dashboards and ML-ready feature stores
✅ Automation and CI/CD workflows

I bring together performance engineering + business outcomes.

🛠️ Tech Stack

Languages / Data: Python, SQL, PostgreSQL
Data Engineering: Spark, Kafka, Airflow, Databricks, S3, Glue, Athena
Data Modeling: Parquet/Delta Lake, ACID, Z-order, partitioning
Analytics + ML (supporting skills): Power BI, Scikit-learn, TensorFlow, Streamlit
DevOps: Docker, Git/GitHub, CI/CD (learning)
Cloud Platforms: AWS (primary), Azure experience, Databricks

🎯 Current Focus

Databricks Delta Lake performance + cost optimization

Spark SQL + distributed compute tuning

Building an AWS-based Data Lake with streaming & batch ingestion

Interview prep: systems design, SQL performance, Spark

Executing & documenting projects for portfolio + Medium articles

📌 Featured Project: Healthcare Data Engineering Pipeline

A production-style pipeline supporting heart disease analytics:

✅ Raw → Bronze → Silver → Gold architecture

✅ S3 + Glue/Athena for governance and SQL querying

✅ Spark transformations + validation checks

✅ Delta Lake for reduced storage + faster downstream queries

✅ Power BI dashboards for physician insights

👉 ML component in progress:
https://github.com/albe290/Heart-Disease-Prediction

Planned Impact: 30–40% ETL runtime reduction through Delta optimizations

🔄 Coming Next: Real-Time Fraud Detection Pipeline

Streaming ingestion with Kafka

Spark Structured Streaming transformations

Delta Lake + Z-order for optimized querying

Real-time fraud alerts with dashboard monitoring

Class-imbalance handling with ML model scoring

Designed to show low-latency analytics with reduced compute cost

📫 Connect with Me

💼 LinkedIn: https://www.linkedin.com/in/aalbertglenn/

✍🏽 Medium: https://medium.com/@AlbertGlenn

🌐 Portfolio: Coming Soon

If you're building data platforms with performance and fiscal responsibility, let’s talk.
I’m open to collaboration and full-time Data Engineering roles — especially in healthcare, sports analytics, and financial intelligence.





