ğŸ‘‹ Hi, I'm Albert Glenn

ğŸ’¾ Data Engineering | âš™ï¸ Pipeline Performance & Cost Optimization | â˜ï¸ Cloud + Streaming | ğŸ“ˆ Data for Business Impact

ğŸš€ About Me

I specialize in building scalable, secure, and cost-efficient data systems that transform raw data into trusted assets for analytics, machine learning, and product innovation.

My unique edge:
I engineer pipelines that reduce warehouse compute + storage spend while improving performance and data availability.

Currently completing the Zero to Mastery Data Engineering Career Path and applying every concept through real-world projects in healthcare and finance.

ğŸ’¡ Business Impact Engineering

I architect pipelines that deliver faster insights at lower cost:

ğŸ“‰ Reduced compute/storage costs with Delta Lake optimization
(partitioning, Z-ordering, file compaction, caching)

âš¡ Faster ETL + query performance â†’ less runtime, fewer cluster hours

ğŸ” Reuse data efficiently via ELT + bronze/silver/gold layers

ğŸ” Governance + schema enforcement â†’ higher quality & compliance

ğŸš€ Improved analytics/ML velocity â†’ quicker data-driven decisions

My priority: enable teams to do more with data while spending less.

ğŸ§± What I Build

âœ… Batch + streaming data pipelines (Kafka, Spark, S3)
âœ… Data lakes with Delta format & Lakehouse architecture (Databricks + AWS)
âœ… Data modeling: OLTP + OLAP + star schemas
âœ… Secure + governed environments (RBAC, .env, IAM)
âœ… Power BI dashboards and ML-ready feature stores
âœ… Automation and CI/CD workflows

I bring together performance engineering + business outcomes.

ğŸ› ï¸ Tech Stack

Languages / Data: Python, SQL, PostgreSQL
Data Engineering: Spark, Kafka, Airflow, Databricks, S3, Glue, Athena
Data Modeling: Parquet/Delta Lake, ACID, Z-order, partitioning
Analytics + ML (supporting skills): Power BI, Scikit-learn, TensorFlow, Streamlit
DevOps: Docker, Git/GitHub, CI/CD (learning)
Cloud Platforms: AWS (primary), Azure experience, Databricks

ğŸ¯ Current Focus

Databricks Delta Lake performance + cost optimization

Spark SQL + distributed compute tuning

Building an AWS-based Data Lake with streaming & batch ingestion

Interview prep: systems design, SQL performance, Spark

Executing & documenting projects for portfolio + Medium articles

ğŸ“Œ Featured Project: Healthcare Data Engineering Pipeline

A production-style pipeline supporting heart disease analytics:

âœ… Raw â†’ Bronze â†’ Silver â†’ Gold architecture

âœ… S3 + Glue/Athena for governance and SQL querying

âœ… Spark transformations + validation checks

âœ… Delta Lake for reduced storage + faster downstream queries

âœ… Power BI dashboards for physician insights

ğŸ‘‰ ML component in progress:
https://github.com/albe290/Heart-Disease-Prediction

Planned Impact: 30â€“40% ETL runtime reduction through Delta optimizations

ğŸ”„ Coming Next: Real-Time Fraud Detection Pipeline

Streaming ingestion with Kafka

Spark Structured Streaming transformations

Delta Lake + Z-order for optimized querying

Real-time fraud alerts with dashboard monitoring

Class-imbalance handling with ML model scoring

Designed to show low-latency analytics with reduced compute cost

ğŸ“« Connect with Me

ğŸ’¼ LinkedIn: https://www.linkedin.com/in/aalbertglenn/

âœğŸ½ Medium: https://medium.com/@AlbertGlenn

ğŸŒ Portfolio: Coming Soon

If you're building data platforms with performance and fiscal responsibility, letâ€™s talk.
Iâ€™m open to collaboration and full-time Data Engineering roles â€” especially in healthcare, sports analytics, and financial intelligence.





